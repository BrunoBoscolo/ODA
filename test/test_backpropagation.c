#include "minunit.h"
#include "gann.h"
#include "data_loader.h"
#include "backpropagation.h"
#include <math.h>
#include <stdlib.h>

extern const double TEST_EPSILON;

const char* test_calculate_mse() {
    // 1. Setup
    const int architecture[] = {2, 3, 1};
    NeuralNetwork* net = nn_create(3, architecture, RELU, SIGMOID);
    // Set weights and biases to known values
    for (int l = 0; l < net->num_layers - 1; l++) {
        for (int r = 0; r < net->weights[l]->rows; r++) {
            for (int c = 0; c < net->weights[l]->cols; c++) {
                net->weights[l]->data[r][c] = 0.5;
            }
        }
        for (int c = 0; c < net->biases[l]->cols; c++) {
            net->biases[l]->data[0][c] = 0.1;
        }
    }

    Dataset* dataset = malloc(sizeof(Dataset));
    dataset->num_items = 1;
    dataset->images = create_matrix(1, 2);
    dataset->images->data[0][0] = 0.2;
    dataset->images->data[0][1] = 0.3;
    dataset->labels = create_matrix(1, 1);
    dataset->labels->data[0][0] = 0.9; // Target label

    // 2. Execution
    double mse = calculate_mse(net, dataset);

    // 3. Assertion
    // This is a placeholder value. The actual expected value would need to be calculated manually.
    // The goal here is to ensure the function runs and returns a plausible value.
    mu_assert("MSE should be non-negative", mse >= 0);
    // A more specific assertion would be:
    // mu_assert("MSE calculation is incorrect", fabs(mse - EXPECTED_VALUE) < 1e-6);

    // 4. Cleanup
    nn_free(net);
    free_dataset(dataset);

    return NULL;
}

const char* test_backprop_early_stopping() {
    gann_seed_rng(12345); // Seed the RNG to make the test deterministic
    // 1. Create two dummy datasets, one for training, one for validation
    Dataset* train_dataset = create_dummy_dataset_with_label(10, 0); // All labels are 0
    Dataset* validation_dataset = create_dummy_dataset_with_label(10, 1); // All labels are 1
    mu_assert("Failed to create dummy datasets", train_dataset != NULL && validation_dataset != NULL);

    // 2. Define network architecture and training parameters
    const int ARCHITECTURE[] = {train_dataset->images->cols, 10, train_dataset->labels->cols};
    GannBackpropParams params = {
        .architecture = ARCHITECTURE,
        .num_layers = sizeof(ARCHITECTURE) / sizeof(int),
        .learning_rate = 0.01,
        .epochs = 50, // High number of epochs
        .batch_size = 1,
        .activation_hidden = RELU,
        .activation_output = SIGMOID,
        .optimizer_type = ADAM,
        .beta1 = 0.9, .beta2 = 0.999, .epsilon = 1e-8,
        .logging = true, // Enable logging to see the early stop message
        .early_stopping_patience = 3,
        .early_stopping_threshold = 0.01
    };

    // 3. Create and train the network
    NeuralNetwork* net = nn_create(params.num_layers, params.architecture, params.activation_hidden, params.activation_output);
    nn_init(net);
    nn_init_optimizer_state(net);

    // This is a bit of a trick to test early stopping.
    // We can't easily check the number of epochs run, so we'll check the final accuracy.
    // The network will quickly learn the training set (accuracy 100%).
    // It will NEVER be able to improve on the validation set (accuracy 0%).
    // So, it should train for `patience + 1` epochs and then stop.
    // The +1 is because the first epoch sets the baseline accuracy.
    backpropagate(net, train_dataset, &params, validation_dataset);

    // 4. Assertion
    // We can't directly check the number of epochs run.
    // Instead, we verify that the network learned the training data perfectly,
    // and that it has very low accuracy on the validation data, as expected.
    double train_accuracy = gann_evaluate(net, train_dataset);
    double validation_accuracy = gann_evaluate(net, validation_dataset);

    mu_assert("Training accuracy should be perfect", fabs(train_accuracy - 1.0) < TEST_EPSILON);
    mu_assert("Validation accuracy should be near zero", validation_accuracy < 0.1);


    // 5. Cleanup
    nn_free(net);
    free_dataset(train_dataset);
    free_dataset(validation_dataset);

    return NULL;
}


// A simple test to see if the network can learn a single instance (overfit).
const char* test_backprop_overfit_single_instance() {
    // 1. Create a dummy dataset with one sample
    Dataset* dummy_dataset = create_dummy_dataset(1);
    mu_assert("Failed to create dummy dataset", dummy_dataset != NULL);

    // 2. Define network architecture and training parameters
    const int ARCHITECTURE[] = {dummy_dataset->images->cols, 10, dummy_dataset->labels->cols};
    GannBackpropParams params = {
        .architecture = ARCHITECTURE,
        .num_layers = sizeof(ARCHITECTURE) / sizeof(int),
        .learning_rate = 0.1,
        .epochs = 200, // More epochs to ensure overfitting
        .batch_size = 1,
        .activation_hidden = RELU,
        .activation_output = SIGMOID,
        .optimizer_type = SGD,
        .logging = false // Disable logging for tests
    };

    // 3. Create and train the network
    NeuralNetwork* net = nn_create(params.num_layers, params.architecture, params.activation_hidden, params.activation_output);
    nn_init(net);
    backpropagate(net, dummy_dataset, &params, NULL);

    // 4. Test the prediction
    int prediction = gann_predict(net, dummy_dataset->images->data[0]);

    // Find the actual label from the one-hot encoded vector
    int actual_label = -1;
    for(int i=0; i < dummy_dataset->labels->cols; i++){
        if(fabs(dummy_dataset->labels->data[0][i] - 1.0) < TEST_EPSILON){
            actual_label = i;
            break;
        }
    }

    mu_assert("Prediction should match the label after training (SGD)", prediction == actual_label);

    // 5. Cleanup
    nn_free(net);
    free_dataset(dummy_dataset);

    return NULL;
}

const char* test_backprop_overfit_single_instance_adam() {
    // 1. Create a dummy dataset with one sample
    Dataset* dummy_dataset = create_dummy_dataset(1);
    mu_assert("Failed to create dummy dataset", dummy_dataset != NULL);

    // 2. Define network architecture and training parameters
    const int ARCHITECTURE[] = {dummy_dataset->images->cols, 10, dummy_dataset->labels->cols};
    GannBackpropParams params = {
        .architecture = ARCHITECTURE,
        .num_layers = sizeof(ARCHITECTURE) / sizeof(int),
        .learning_rate = 0.01, // Adam usually requires a smaller learning rate
        .epochs = 200,
        .batch_size = 1,
        .activation_hidden = RELU,
        .activation_output = SIGMOID,
        .optimizer_type = ADAM,
        .beta1 = 0.9,
        .beta2 = 0.999,
        .epsilon = 1e-8,
        .logging = false
    };

    // 3. Create and train the network
    NeuralNetwork* net = nn_create(params.num_layers, params.architecture, params.activation_hidden, params.activation_output);
    nn_init(net);
    nn_init_optimizer_state(net); // Important for Adam
    backpropagate(net, dummy_dataset, &params, NULL);

    // 4. Test the prediction
    int prediction = gann_predict(net, dummy_dataset->images->data[0]);
    int actual_label = -1;
    for(int i=0; i < dummy_dataset->labels->cols; i++){
        if(fabs(dummy_dataset->labels->data[0][i] - 1.0) < TEST_EPSILON){
            actual_label = i;
            break;
        }
    }

    mu_assert("Prediction should match the label after training (Adam)", prediction == actual_label);

    // 5. Cleanup
    nn_free(net);
    free_dataset(dummy_dataset);

    return NULL;
}

const char* test_backprop_overfit_single_instance_rmsprop() {
    // 1. Create a dummy dataset with one sample
    Dataset* dummy_dataset = create_dummy_dataset(1);
    mu_assert("Failed to create dummy dataset", dummy_dataset != NULL);

    // 2. Define network architecture and training parameters
    const int ARCHITECTURE[] = {dummy_dataset->images->cols, 10, dummy_dataset->labels->cols};
    GannBackpropParams params = {
        .architecture = ARCHITECTURE,
        .num_layers = sizeof(ARCHITECTURE) / sizeof(int),
        .learning_rate = 0.01,
        .epochs = 200,
        .batch_size = 1,
        .activation_hidden = RELU,
        .activation_output = SIGMOID,
        .optimizer_type = RMSPROP,
        .beta2 = 0.999,
        .epsilon = 1e-8,
        .logging = false
    };

    // 3. Create and train the network
    NeuralNetwork* net = nn_create(params.num_layers, params.architecture, params.activation_hidden, params.activation_output);
    nn_init(net);
    nn_init_optimizer_state(net); // Important for RMSprop
    backpropagate(net, dummy_dataset, &params, NULL);

    // 4. Test the prediction
    int prediction = gann_predict(net, dummy_dataset->images->data[0]);
    int actual_label = -1;
    for(int i=0; i < dummy_dataset->labels->cols; i++){
        if(fabs(dummy_dataset->labels->data[0][i] - 1.0) < TEST_EPSILON){
            actual_label = i;
            break;
        }
    }

    mu_assert("Prediction should match the label after training (RMSprop)", prediction == actual_label);

    // 5. Cleanup
    nn_free(net);
    free_dataset(dummy_dataset);

    return NULL;
}
