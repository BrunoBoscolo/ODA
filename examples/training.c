#include <stdio.h>
#include <stdlib.h>
#include <time.h>
#include "gann.h"
#include "utils.h"

int main() {
    // Seed the random number generator.
    // Using a fixed seed (e.g., 12345) makes the training process deterministic,
    // which is useful for debugging and for comparing different training runs.
    // To get different results on each run, you could use: gann_seed_rng(time(NULL));
    gann_seed_rng(12345);

    printf("--- Starting MNIST Training with the GANN Simple API ---\n");

    // --- 1. Load MNIST Data ---
    // The data loader is not part of the core GANN library, so it doesn't use the new error system.
    // We will check for errors in the traditional C way (checking for NULL).
    const char* data_prefix = find_data_path_prefix();
    char train_images_path[256];
    char train_labels_path[256];
    snprintf(train_images_path, sizeof(train_images_path), "%s%s", data_prefix, "train-images.idx3-ubyte");
    snprintf(train_labels_path, sizeof(train_labels_path), "%s%s", data_prefix, "train-labels.idx1-ubyte");

    Dataset* train_dataset = load_mnist_dataset(train_images_path, train_labels_path);
    if (!train_dataset) {
        // Since data_loader doesn't use gann_errors, we print a generic message.
        fprintf(stderr, "Error: Failed to load training data. Check file paths and integrity.\n");
        return 1;
    }

    // --- 2. Define Training Parameters ---
    // For this example, we will use the convenient `gann_create_default_params`
    // function to get a struct with sensible default values.
    GannTrainParams params = gann_create_default_params();

    // The two most important parameters that MUST be set by the user are the
    // network architecture and the number of layers.
    const int ARCHITECTURE[] = {MNIST_IMAGE_SIZE, 128, 64, MNIST_NUM_CLASSES};
    params.architecture = ARCHITECTURE;
    params.num_layers = sizeof(ARCHITECTURE) / sizeof(int);

    // We can also override any of the default parameters if we want to experiment.
    // For example, let's use a different activation function for the hidden layers
    // and run for fewer generations for a quicker example.
    params.activation_hidden = LEAKY_RELU;
    params.num_generations = 10; // Default is 100

    // --- New: Early Stopping ---
    // We can enable early stopping to prevent overfitting and save time.
    // The training will stop if the validation accuracy doesn't improve by at least
    // `early_stopping_threshold` for `early_stopping_patience` generations.
    // A validation set is required for this. For this simple example, we'll
    // leave it disabled (patience = 0).
    params.early_stopping_patience = 10;
    params.early_stopping_threshold = 0.01; // 1% improvement required

    // Print the final parameters to the console.
    printf("Network architecture: [");
    for (int i = 0; i < params.num_layers; i++)
        printf("%d%s", params.architecture[i], i == params.num_layers - 1 ? "" : ", ");
    printf("]\n");
    printf("Generations: %d | Population: %d | Mutation Chance: %.2f%%\n",
           params.num_generations, params.population_size, params.mutation_chance * 100);


    // --- 3. Split Data (Optional) & Run Training ---
    // For this example, we'll split the training data to create a validation set
    // so we can demonstrate the early stopping feature.
    Dataset* validation_dataset = malloc(sizeof(Dataset));
    Dataset* new_train_dataset = malloc(sizeof(Dataset));

    // Let's use 10,000 samples for validation
    split_dataset(train_dataset, 10000, new_train_dataset, validation_dataset);

    printf("Training data: %d samples | Validation data: %d samples\n", new_train_dataset->num_items, validation_dataset->num_items);

    // This single function call encapsulates the entire genetic algorithm process.
    printf("--------------------\n");
    printf("Starting training...\n");
    // Pass the validation set to the training function.
    NeuralNetwork* best_net = gann_train(&params, new_train_dataset, validation_dataset);

    // --- 4. Check for errors and Save the Best Network ---
    if (best_net) {
        printf("Training complete.\n");
        if (nn_save(best_net, "trained_network.dat")) {
            printf("Best network saved to trained_network.dat\n");
        } else {
            // If saving fails, get the specific error from the GANN library.
            GannError err = gann_get_last_error();
            fprintf(stderr, "Error: Failed to save the best network. Reason: %s\n", gann_error_to_string(err));
        }
        nn_free(best_net);
    } else {
        // If training fails, get the specific error from the GANN library.
        GannError err = gann_get_last_error();
        fprintf(stderr, "Error: Training failed. Reason: %s\n", gann_error_to_string(err));
    }

    // --- 5. Cleanup ---
    free_dataset(train_dataset);
    free_dataset(new_train_dataset);
    free_dataset(validation_dataset);


    return 0;
}
